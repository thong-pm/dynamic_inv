<!DOCTYPE HTML>
<html>
<head>
    <title>Dynamic Inventory Dashboard</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="assets/css/main.css" />
</head>
<body>
    <header id="header">
        <nav id="toc">
            <ul>
                <h3><a href="#title">Table of Content</a></h3>
                <li><a href="#heading1">Introduction</a></li>
                <li><a href="#heading2">Pipeline</a></li>
                <ul>
                    <li><a href="#subheading4">Motivation</li>
                    <li><a href="#subheading5">Data Relation Table</li>
                </ul>
                <li><a href="#heading3">Dashboard</a></li>
                <ul>
                    <li><a href="#subheading6">Dashboard</li>
                    <li><a href="#subheading7">Benefits</li>
                    <li><a href="#subheading8">Breakdown</li>
                </ul>
                <li><a href="#heading4">Improvements</a></li>
                <li><a href="#heading5">Conclusion</a></li>
            <br>
            <a href="https://thong-pm.github.io/" target="_blank" ><strong>Main Portfolio</strong></a>
        </nav>
    </header>
    <div id="main">
        <h1 id ="title"><strong>Power BI Dynamic Inventory Dashboard</strong></h1>
        <h2 id="heading1"><strong>Introduction</strong></h2>
        <h3 id="subheading1"><strong>Context</strong></h3>
        <p> I had a fulfilling time working as a Demand Planner at a company specializing in Power Technology and Services. Per the standard process, the data was hybrid recorded and maintained using Excel workbooks (clouded on Sharepoint) and SAP S4/HANA.</p>
        <p> The problem was the lack of information centralization. Checking and tracking the performance was required to access multiple different data points. Also, this provides zero help in transparency between the Supply Chain team and other stakeholders. </p>
        <p> Per the traditional way, I used a lot of INDEX MATCH in Excel, trying to gather all the relevant information across multiple data points. It did not go well. At one point, my workbook took 15 minutes just to open.</p>
        <p> In short, there was a lot of room for improvement regarding my data management.</p>
        <h3 id="subheading2"><strong>Scope</strong></h3>
        <p> To centralize the information of POs, SOs, availability, and schedule shipping from SAP HANA plus multiple Excel sheets into a single dashboard for tracking stock level by the pre-defined timeline.</p>
        <p> The pipeline is based on the corporation's facilities and resources at that time, which to my knowing, was not too unique.</p>
        <h3 id="subheading3"><strong>Deliverable</strong></h3>
        <p> Eventually, a dashboard was come up as a solution. The <a href="#heading3">Dynamic Inventory Level Dashboard</a> was one of the attempts to display the inventory information from various data points.</p>
        <p>Although the solution were never further implemented before I left, I know the dashboard can solve some problems I was having back then.</p>
        <br>
        <i> Disclaimer:<br>
            <ul>
                <li> The datasets were censored and edited.</li>
                <li> The dashboard in this blog is a prototype.</li>
            </ul> 
        </i>
        <br>
        <p><a href="https://github.com/thong-pm/Data_Port/tree/main/Logistics%20Data%20Pipeline" target="_blank" >Link to the resources of this project</a></p>
        <br><!-- ___ -->
        <h2 id="heading2"><strong>Pipeline</strong></h2>
        <iframe src="assets/viz/workflow.html" width="1000" height="650" frameborder="0"></iframe>
        <figcaption>Project Data Pipeline Diagram</figcaption>
        <br>
        <h3 id="subheading4"><strong>Motivation Behind The Pipeline</strong></h3>
        <h4><strong>From Sources to Data Warehouse (Task 1 → Task 3) </strong></h4>
        <p> The direct access or API to SAP HANA for users like me was restricted.</p>
        <p> The solution would be collaboration. The on-demand normalized datasets are processed by a dedicated BI team and mailed via Outlook based on the time aligned.</p>
        <p> And, as mentioned, there was relevant data recorded on Excel workbooks which I do have direct access.</p> 
        <p> Then, those normalized datasets then needs to be stored together somewhere after the transformation. The most common solution is an SQL Data Warehouse.</p>
        <p> For this prototype run, I use SQL Server as my local data storing solution, this RDBMS is robust enough to act as a data warehouse. And ... it's free.</p>
        <p> Normalization (mostly hadling date format) and bulk upload into SQL Server was handled by a <a href="https://github.com/thong-pm/Data_Port/blob/3ff81ccafc1e2a3c574b950cd28c1db15cd70bd8/Logistics%20Data%20Pipeline/script/script_bulk_upload.ipynb" target="_blank" >Python Script</a>.</p> 
        <h4><strong>From Data Warehouse to Power BI (Task 3 → Task 5)</strong></h4>
        <p> Bascally, I need to see at that particular <strong>date</strong>, at that particular <strong>plant</strong>, what is the <strong>quantities</strong> of that particular <strong>material code</strong>.</p> 
        <p>The quantities basic math is: Current Stock + PO quantities - SO quantities.
            This means the key is a composition of 3 columns: <strong>date</strong>, <strong>plant</strong>, and <strong>material code</strong>.</p>
        <p> I was standing between creating the composite key column for all the fact tables( date & plant & material_code in Power Query) or "Merge" the quantity info of the fact tables (Current Stock, SO, PO) into one big table.</p>
        <p> <a href = "https://learn.microsoft.com/en-us/power-bi/guidance/star-schema">Power BI loves star schema</a>, everything is built around it. Merging into one big fact table was my choice for this prototype run. Concerns regarding scalability and performance are raised, but considering the composite key columns will require calculation in the row level, I do not think it will provide any significant advantages if it was implemented instead.</p>
        <p> So far so good it’s working on Power BI Service resources. Around 5 million rows for 2 years span (730 days) with around 1500 active material codes and 2 plants.</p>
        <br>
        <h5><strong>What does it really means?</strong></h5>
        <p><i><a href="https://ssbipolar.com/2021/05/31/roches-maxim/" target="_blank" > “Data should be transformed as far upstream as possible and as far downstream as necessary.”</a></i>
         Shout out to Matthew Roche.</p>
        <i>All possible transformations in data warehouse instead of Power Query:</i>
        <p>I cross-joined the date_key, materials, and plant as the “skeleton” for the dataset. Resulted in the new table: Dim_ Cartesian in SQL Server. The intention was use this skeleton and left-joined with the transactional data from multiple fact tables.</p> 
        <p>Eventually creating one big central fact table.</p> 
        <p>But I wouldn't join them in SQL Server, at least not with this prototype pipeline, because it requires orchestra tools to ensure every future changes in the small fact tables would be updated in the big fact table, I used Power Query further down the stream instead.</p>
        <figure>
            <img src="assets/viz/cross-join-diagram.png" alt="Venn diagram of elements for an eligible project" width="500">
            <figcaption>The Cross-join of Dimension Tables</figcaption>
        </figure> 
        <i>All possible transformations in Power Query instead of DAX:</i>
        <p>All relevant tables loaded into Power BI.</p>
        <p><strong>Merge Query</strong> using Dim_Cartasian columns as composite keys to join quantities of Current Stock, POs (incomming), and SOs (outgoing). This big fact table is named Closing_Inventory.</p>
        <p>Power Query helps updating every changes in Current Stock, POs, or SOs into the Closing_Inventory table.</p>
        <p>The data model at this point seems <a href="#subheading2">star schema</a> in my opinion. Not an orthodox modeling but in the form that's can be utilized with Power BI. Explicit the one central big fact table and surrounding dimension tables.</p>
        <p>- Dimension tables support filtering and grouping.</p>
        <p>- The fact table supports summarization of the numbers.</p>
        <p>In this case, filtering applies for 3 main dimensions: <strong>date</strong>, <strong>plant</strong>, and <strong>material code</strong>. As for the sumarization of numbers, the central fact table, <strong>Closing_Inventory</strong> helps the DAX straightforward.</p>
        <i>Measurements in DAX:</i>
        <pre><code>Stock Level = 
            VAR Stock = [Innital Stock] + [In] -[Out]
            RETURN
            Stock</code></pre>
        <pre><code>Innital Stock = CALCULATE(SUM(Closing_Inventory[Qty]),'Closing_Inventory'[date_key] <= MAX(dim_date[date_key]))</code></pre>
        <pre><code>In = CALCULATE(SUM(Closing_Inventory[PO_Qty]), 'Closing_Inventory'[date_key] <= MAX(dim_date[date_key])) </code></pre>
        <pre><code>Out = CALCULATE(SUM(Closing_Inventory[SO_Qty]), 'Closing_Inventory'[date_key] <= MAX(dim_date[date_key]))</code></pre>
        <br>
        <p><a href="https://github.com/thong-pm/Data_Port/tree/55968a4d64567380ab5d1964d7331db23b03d680/Logistics%20Data%20Pipeline" target="_blank" >Link to the PBIX file</a>.</p>
        <br>
        <h3 id="subheading5"><strong>Data Table Relation</strong></h3>
        <iframe src="assets/viz/table_rel.html" width="1000" height="500" frameborder="0"></iframe>
        <figcaption>Entity-Relationship Diagram</figcaption>
        <br><!-- ___ -->
        <br><!-- ___ -->
        <h2 id="heading3"><strong>Dynamic Inventory Level Dashboard (Interactive)</strong></h2>
        <h3 id="subheading6"><strong>Dashboard</strong></h3>
        <iframe title="Dynamic Inventory Level Dashboard" width="1050" height="750" src="https://app.powerbi.com/view?r=eyJrIjoiZDFmOGY3ZjAtYjhkOC00YzIwLTk2ZTYtYWRkMDI2M2I1MGJhIiwidCI6Ijk0YzBmYWUxLWY5MDEtNDMwZi05ZTkyLWJiMGZkNzMxZTlmNCIsImMiOjEwfQ%3D%3D" frameborder="0" allowFullScreen="true"></iframe>
        <figcaption>Dynamic Inventory Level Dashboard</figcaption>
        <br>
        <h3 id="subheading7"><strong>Main Benefits</strong></h3>
        <p>- User-friendly centralized display of data from multiple sources.</p>
        <p>- Transparency helps cut off cross-checking and query time between the stakeholders.</p>
        <p>- Enhance situation awareness for the planner and relevant stakeholders.</p>
        <br>
        <h3 id="subheading8"><strong>Breakdown</strong></h3>
        <iframe src="assets/viz/elaborate.html" width="1100" height="800" frameborder="0"></iframe>
        <figcaption>Break Down of the Visualizations in the Dashboard</figcaption>
        <h2 id="heading4"><strong>Improvements and Expansion</strong></h2>
        <p>There's a lot of room for improvement regarding the automation of the pipeline or enhancing the situation awareness capability of this solution.</p>
        <p>Aim to save time by cut down the manual tasks. Also, highlight the backorders risk in the time window that’s can make the impact for the Planner to expedite with suppllier or for the Sales team to re-negotiate the timeline with customers.</p>
        <h3 id="subheading9"><strong>The Pipeline</strong></h3>
        <p>- For Task 2 and Task 4, incorporate orchestra tools to reduce manual tasks and more sustainable in handling data updating.</p>
        <p>- Upstream the data combination to enhance the scalability and the flexibility to set up new tracking measures.</p>
        <h3 id="subheading10    "><strong>The Dashboard</strong></h3>
        <p>- Backorders date highlights/warning mechanic.</p>
        <p>- Implement calculation of the re-order point for each material for each plant.</p>
        <h2 id="heading5"><strong>Conclusion</strong></h2>
        <figure>
        <img src="assets/viz/venn_project_eligible.png" alt="Venn diagram of elements for an eligible project" width="320">
        <figcaption>Venn Diagram of the Elements for an Eligible Project</figcaption>
        </figure>
        <p>For it to work in a business setting, lots of aspects must be brought to the table.</p>
        <p>Regarding this blog, the main focus will be the <strong>feasible (technology)</strong>. The motivation utilizing the knowing of the company's facitiliies and the technical skills trying to tackle the challenges I'd encoutered as a Demand Planner.</p>
        <p>With that being said, there’s a lot of room for improvement for sure.</p>
        <hr>
        <ul>
            <li><p><a href="https://github.com/thong-pm/Data_Port/tree/main/Logistics%20Data%20Pipeline" target="_blank" ><strong>Resources of this project</strong></a></p></li>
            <li><p><a href="https://thong-pm.github.io/" target="_blank" ><strong>Main Portfolio</strong></a></p></li>
        </ul>
    </div>
</body> 
</html> 
